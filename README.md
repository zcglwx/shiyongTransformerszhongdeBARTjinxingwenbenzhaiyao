# 使用Transformers中的BART进行文本摘要

本仓库提供了一个基于Transformers库的BART模型进行文本摘要的代码示例。该示例使用中文版本的BART模型，并使用NLPCC2017的中文摘要数据集进行训练和测试。

## 内容概述

- **包引入**: 导入了必要的Python库，如torch、datasets、lawrouge等。
- **定义参数**: 设置了训练参数，包括batch size、epochs、最大输入和输出长度等。
- **加载数据**: 从NLPCC2017数据集中加载数据，并使用BERT的tokenizer进行分词。
- **数据处理**: 对数据进行预处理，包括调整数据格式、划分数据集、tokenization等。
- **加载模型**: 加载预训练的BART模型，并进行模型训练。
- **模型训练**: 定义了评估函数、训练参数，并使用Seq2SeqTrainer进行模型训练。
- **生成摘要**: 使用训练好的模型生成文本摘要，并输出结果。

## 使用方法

1. **克隆仓库**: 
   ```bash
   git clone https://github.com/yourusername/yourrepository.git
   cd yourrepository
   ```

2. **安装依赖**:
   ```bash
   pip install -r requirements.txt
   ```

3. **运行代码**:
   ```bash
   python main.py
   ```

## 数据集

本示例使用NLPCC2017的中文摘要数据集，数据集位于百度网盘，提取码为knci。

## 模型

使用的是fnlp/bart-base-chinese预训练模型，该模型在中文文本摘要任务上表现良好。

## 结果

训练完成后，模型将生成文本摘要，并输出到控制台。可以通过调整参数和数据集进一步优化模型性能。

## 贡献

欢迎提交问题和改进建议，或者直接提交Pull Request。

## 许可证

本项目遵循CC 4.0 BY-SA版权协议。

## 下载链接
[使用Transformers中的BART进行文本摘要](https://pan.quark.cn/s/0f132790aef8) 

(备用: [备用下载](https://pan.baidu.com/s/1Kj8BDM2r0OrKHVgeA8CWoQ?pwd=1234))

## 说明

该仓库仅用于学习交流，请勿用于商业用途。
